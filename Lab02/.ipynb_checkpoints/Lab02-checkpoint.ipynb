{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - Expansão de Consultas\n",
    "### Hadrizia Santos\n",
    "Nesta atividade será exercitada a noção de expansão de consultas. Considerando a coleção de notícias do lab passado, deve-se executar os seguintes passos:\n",
    "\n",
    "1. Escrever uma função que receba uma coleção de documentos e retorne uma matrix de termos-termos contendo as frequências de co-ocorrência de duas palavras consecutivas no texto (bigramas).\n",
    "2. Escrever uma função que receba um certo termo de consulta e a matriz construída no passo 1 acima e retorneas top-3 palavras em ordem decrescente de frequencia.\n",
    "3. Expandir a consulta original com os termos retornados no passo 2 acima.\n",
    "4. Fazer uma busca disjuntiva (OR) considerando a nova consulta.\n",
    "\n",
    "E responder às perguntas:\n",
    "\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n",
    "\n",
    "## Importar bibliotecas e os dados necessários\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from nltk import bigrams   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy.sparse as sps\n",
    "import math\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "\n",
    "#nltk.download(\"stopwords\") \n",
    "\n",
    "FILE_PATH = '../data/estadao_noticias_eleicao.csv'\n",
    "\n",
    "df = pd.read_csv(FILE_PATH, encoding = 'utf-8')\n",
    "df = df.replace(np.NAN, \"\")\n",
    "\n",
    "# criação de uma nova coluna para a junção do título da notícia com seu conteúdo\n",
    "df['noticia'] = df.titulo + ' ' + df.subTitulo + ' ' + df.conteudo\n",
    "\n",
    "dictionary = collections.defaultdict(list)\n",
    "OR = 'or'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construir matrix de ocorrência\n",
    "**Obs:** O código abaixo que constrói a matriz de ocorrência foi copiado do repositório que está disponível em: https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# O código abaixo está disponivel em: \n",
    "# https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb\n",
    "\n",
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index\n",
    "\n",
    "def  generate_tokens_dataframe(noticia):\n",
    "    # Removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_lists = noticia.apply(lambda text: tokenizer.tokenize(text.lower()))\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stopword_ = stopwords.words('portuguese')\n",
    "    filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])\n",
    "    \n",
    "    tokens = [token for tokens_list in filtered_tokens for token in tokens_list]\n",
    "    return tokens\n",
    "\n",
    "def  generate_tokens_text(text):\n",
    "    # Removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_lists = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stopword_ = stopwords.words('portuguese')\n",
    "    filtered_tokens = [token for token in tokens_lists if token not in stopword_]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = generate_tokens_dataframe(df.noticia)\n",
    "matrix, vocab = co_occurrence_matrix(tokens)\n",
    "consultable_matrix = matrix.tocsr()\n",
    "inverted_vocab = {vocab[key]: key for key in vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retornar as top-3 palavras mais frequentes de acordo com a matriz de ocorrência\n",
    "Escrever uma função que receba uma palavra do dicionário e retorne quais as 3 palavras que ocorrem mais frequentemente com o input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_co_ocurrences(co_matrix, inv_vocab, term, N=3):\n",
    "    # sparse to dense representation\n",
    "    np_array = np.reshape(co_matrix[term].toarray(), -1)\n",
    "    # get indice of N occurences\n",
    "    return list(OrderedDict({inv_vocab[idx]: np_array[idx] for idx in (-np_array).argsort()[:N]}).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expandir a consulta original com os termos retornados no passo 2 acima.\n",
    "Escrever uma função que receba a consulta, pega as 3 palavras que aparecem com maior frequência junto com esse termo e retorne uma consulta expandida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_query(terms):\n",
    "    query = []\n",
    "    terms = terms.split(' ')\n",
    "    if len(terms) > 1:\n",
    "            for term in terms:\n",
    "                query.append(term)\n",
    "                query.extend(get_co_ocurrences(consultable_matrix, inverted_vocab, vocab[term], 3))\n",
    "    else:\n",
    "        query.append(terms[0])\n",
    "        query.extend(get_co_ocurrences(consultable_matrix, inverted_vocab, vocab[terms[0]], 3))\n",
    "        \n",
    "    # removing duplicates terms\n",
    "    return \" \".join(str(x) for x in set(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fazer uma busca disjuntiva (OR) considerando a nova consulta.\n",
    "Escrever uma função que recebe uma consulta de N termos e retorna uma consula disjuntiva entre os termos. O modelo utilizado será a busca binária, que foi implementado em atividades anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_indexes(tokens, docId):\n",
    "    for word in tokens:\n",
    "        if docId not in dictionary[word]:\n",
    "            dictionary[word].append(docId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating inverted index, TF and IDF\n",
    "for index, row in df.iterrows():  \n",
    "    tokens = generate_tokens_text(row.noticia)\n",
    "    create_indexes(tokens, row.idNoticia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_search(terms):\n",
    "    result = set(dictionary[terms[0]])\n",
    "    if (len(terms) > 1):\n",
    "        for term in terms[1:]:\n",
    "            result = result.union(set(dictionary[term]))\n",
    "    return list(result)\n",
    "\n",
    "def search(search):\n",
    "    search = search.lower().split(' ')\n",
    "    if len(search) >= 1:\n",
    "        terms_list = []\n",
    "        for element in search:\n",
    "            if element != OR:\n",
    "                terms_list.append(element)\n",
    "        return or_search(terms_list)\n",
    "    else:\n",
    "        raise ValueError('search must have at least a word.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respondendo às perguntas\n",
    "### Quais os termos retornados para a expansão de cada consulta?\n",
    "As consultas a serem testadas são as seguintes: 'corrupção', 'segundo turno' e 'empresa petrobrás'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query original: segundo turno\n",
      "Query expandida: turno lugar segundo eleição mandato é eleições\n",
      "Query original: lava jato\n",
      "Query expandida: jato é polícia lato lava deflagrada porque\n",
      "Query original: projeto lei\n",
      "Query expandida: anistia responsabilidade lei político ficha poder projeto\n",
      "Query original: compra voto\n",
      "Query expandida: pasadena voto votos presidente distrital dilma refinaria compra\n",
      "Query original: ministério público\n",
      "Query expandida: é público estadual saúde fazenda federal ministério\n"
     ]
    }
   ],
   "source": [
    "queries = ['segundo turno', 'lava jato', 'projeto lei', 'compra voto', 'ministério público']\n",
    "exp_queries = []\n",
    "for query in queries:\n",
    "    ext_query = expand_query(query)\n",
    "    exp_queries.append(ext_query)\n",
    "    print('Query original: %s' % query)\n",
    "    print('Query expandida: %s' % ext_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "Acredito que os termos estejam relacionados com a consulta original nas duas últimas consultas, pois quando se fala em segundo turno, é normalmente sobre eleições e todos os termos estão relacionados ao contexto. O mesmo ocorre para petrobrás, onde os demais termos se relacionam com a consulta. Já a primeira consulta expandida me surpreendeu um pouco, pois não esperava que petrobrás aparecesse tanto junto de corrupção. Outra coisa que observei é que o termo '**é**', que deveria ser considerada stopword, pois apareceu em todas as consultas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "A consulta utilizada será uma busca binária, implementada no Lab1 desta disciplina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compra voto\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(generate_tokens_text('compra de voto')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#rint(search('poucos OR meses'))\n",
    "print(search('poucos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
