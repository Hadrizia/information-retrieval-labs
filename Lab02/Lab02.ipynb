{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - Expansão de Consultas\n",
    "### Hadrizia Santos\n",
    "Nesta atividade será exercitada a noção de expansão de consultas. Considerando a coleção de notícias do lab passado, deve-se executar os seguintes passos:\n",
    "\n",
    "1. Escrever uma função que receba uma coleção de documentos e retorne uma matrix de termos-termos contendo as frequências de co-ocorrência de duas palavras consecutivas no texto (bigramas).\n",
    "2. Escrever uma função que receba um certo termo de consulta e a matriz construída no passo 1 acima e retorneas top-3 palavras em ordem decrescente de frequencia.\n",
    "3. Expandir a consulta original com os termos retornados no passo 2 acima.\n",
    "4. Fazer uma busca disjuntiva (OR) considerando a nova consulta.\n",
    "\n",
    "E responder às perguntas:\n",
    "\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n",
    "\n",
    "## Importar bibliotecas e os dados\n",
    "O primeiro passo é importar as bibliotecas e os dados necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from nltk import bigrams   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy.sparse as sps\n",
    "import math\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "\n",
    "#nltk.download(\"stopwords\") \n",
    "\n",
    "FILE_PATH = '../data/estadao_noticias_eleicao.csv'\n",
    "\n",
    "df = pd.read_csv(FILE_PATH, encoding = 'utf-8')\n",
    "df = df.replace(np.NAN, \"\")\n",
    "\n",
    "# criação de uma nova coluna para a junção do título da notícia com seu conteúdo\n",
    "df['noticia'] = df.titulo + ' ' + df.subTitulo + ' ' + df.conteudo\n",
    "\n",
    "dictionary = collections.defaultdict(list)\n",
    "OR = 'or'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construir matrix de ocorrência\n",
    "**Obs:** O código abaixo que constrói a matriz de ocorrência foi copiado do repositório que está disponível em: https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# O código abaixo está disponivel em: \n",
    "# https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb\n",
    "\n",
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index\n",
    "\n",
    "def  generate_tokens_dataframe(noticia):\n",
    "    # Removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_lists = noticia.apply(lambda text: tokenizer.tokenize(text.lower()))\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stopword_ = stopwords.words('portuguese')\n",
    "    filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])\n",
    "    \n",
    "    tokens = [token for tokens_list in filtered_tokens for token in tokens_list]\n",
    "    return tokens\n",
    "\n",
    "def  generate_tokens_text(text):\n",
    "    # Removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_lists = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stopword_ = stopwords.words('portuguese')\n",
    "    filtered_tokens = [token for token in tokens_lists if token not in stopword_]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = generate_tokens_dataframe(df.noticia)\n",
    "matrix, vocab = co_occurrence_matrix(tokens)\n",
    "consultable_matrix = matrix.tocsr()\n",
    "inverted_vocab = {vocab[key]: key for key in vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retornar as top-3 palavras mais frequentes de acordo com a matriz de ocorrência\n",
    "Escrever uma função que receba uma palavra do dicionário e retorne quais as 3 palavras que ocorrem mais frequentemente com o input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_co_ocurrences(co_matrix, inv_vocab, term, N=3):\n",
    "    # sparse to dense representation\n",
    "    np_array = np.reshape(co_matrix[term].toarray(), -1)\n",
    "    # get indice of N occurences\n",
    "    return list(OrderedDict({inv_vocab[idx]: np_array[idx] for idx in (-np_array).argsort()[:N]}).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expandir a consulta original com os termos retornados no passo 2 acima.\n",
    "Escrever uma função que receba a consulta, pega as 3 palavras que aparecem com maior frequência junto com esse termo e retorne uma consulta expandida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_query(terms):\n",
    "    query = []\n",
    "    terms = terms.split(' ')\n",
    "    if len(terms) > 1:\n",
    "            for term in terms:\n",
    "                query.append(term)\n",
    "                query.extend(get_co_ocurrences(consultable_matrix, inverted_vocab, vocab[term], 3))\n",
    "    else:\n",
    "        query.append(terms[0])\n",
    "        query.extend(get_co_ocurrences(consultable_matrix, inverted_vocab, vocab[terms[0]], 3))\n",
    "        \n",
    "    # removing duplicates terms\n",
    "    return \" \".join(str(x) for x in set(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fazer uma busca disjuntiva (OR) considerando a nova consulta.\n",
    "Escrever uma função que recebe uma consulta de N termos e retorna uma consula disjuntiva entre os termos. O modelo utilizado será a busca binária, que foi implementado em atividades anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_indexes(tokens, docId):\n",
    "    for word in tokens:\n",
    "        if docId not in dictionary[word]:\n",
    "            dictionary[word].append(docId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating inverted index\n",
    "for index, row in df.iterrows():  \n",
    "    tokens = generate_tokens_text(row.noticia)\n",
    "    create_indexes(tokens, row.idNoticia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def or_search(terms):\n",
    "    result = set(dictionary[terms[0]])\n",
    "    for term in terms[1:]:\n",
    "        result = result.union(set(dictionary[term]))\n",
    "    return list(result)\n",
    "\n",
    "def search(search):\n",
    "    search = search.lower().split(' ')\n",
    "    if len(search) >= 1:\n",
    "        terms_list = []\n",
    "        for element in search:\n",
    "            if element != OR:\n",
    "                terms_list.append(element)\n",
    "        return or_search(terms_list)\n",
    "    else:\n",
    "        raise ValueError('search must have at least a word.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respondendo às perguntas\n",
    "### Quais os termos retornados para a expansão de cada consulta?\n",
    "As consultas a serem testadas são as seguintes: 'segundo turno', 'lava jato', 'projeto de lei', 'compra de voto' e 'empresa petrobrás'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query original: segundo turno\n",
      "Query expandida: é turno segundo eleição mandato lugar eleições\n",
      "Query original: lava jato\n",
      "Query expandida: é lava deflagrada lato jato porque polícia\n",
      "Query original: projeto de lei\n",
      "Query expandida: ficha político poder lei anistia projeto responsabilidade\n",
      "Query original: compra de voto\n",
      "Query expandida: pasadena dilma refinaria voto votos distrital presidente compra\n",
      "Query original: ministério público\n",
      "Query expandida: estadual é ministério saúde fazenda público federal\n"
     ]
    }
   ],
   "source": [
    "queries = ['segundo turno', 'lava jato', 'projeto de lei', 'compra de voto', 'ministério público']\n",
    "exp_queries = []\n",
    "for query in queries:\n",
    "    ext_query = expand_query(process_query(query))\n",
    "    exp_queries.append(ext_query)\n",
    "    print('Query original: %s' % query)\n",
    "    print('Query expandida: %s' % ext_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "Acredito que os termos estejam relacionados com a consulta original nas duas últimas consultas, pois quando se fala em segundo turno, é normalmente sobre eleições e todos os termos estão relacionados ao contexto. O mesmo ocorre para petrobrás, onde os demais termos se relacionam com a consulta. Já a primeira consulta expandida me surpreendeu um pouco, pois não esperava que petrobrás aparecesse tanto junto de corrupção. Outra coisa que observei é que o termo '**é**', que deveria ser considerada stopword, pois apareceu em todas as consultas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "A consulta utilizada será uma busca binária, implementada no Lab1 desta disciplina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process query\n",
    "def process_query(query):\n",
    "    return ' '.join(generate_tokens_text(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para a consulta \"segundo turno\", a busca retornou 4448 documentos.\n",
      "Para a consulta expandida \"é turno segundo eleição mandato lugar eleições\", a busca retornou 7368 documentos.\n",
      "Para a consulta \"lava jato\", a busca retornou 530 documentos.\n",
      "Para a consulta expandida \"é lava deflagrada lato jato porque polícia\", a busca retornou 7023 documentos.\n",
      "Para a consulta \"projeto de lei\", a busca retornou 1971 documentos.\n",
      "Para a consulta expandida \"ficha político poder lei anistia projeto responsabilidade\", a busca retornou 4965 documentos.\n",
      "Para a consulta \"compra de voto\", a busca retornou 1659 documentos.\n",
      "Para a consulta expandida \"pasadena dilma refinaria voto votos distrital presidente compra\", a busca retornou 6376 documentos.\n",
      "Para a consulta \"ministério público\", a busca retornou 2474 documentos.\n",
      "Para a consulta expandida \"estadual é ministério saúde fazenda público federal\", a busca retornou 7320 documentos.\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "exp_result = []\n",
    "for query in queries:\n",
    "    result.append(search(process_query(query)))\n",
    "for query in exp_queries:\n",
    "    exp_result.append(search(process_query(query)))\n",
    "    \n",
    "for i in range(len(result)):\n",
    "    print('Para a consulta \"%s\", a busca retornou %d documentos.' % (queries[i], len(result[i])))\n",
    "    print('Para a consulta expandida \"%s\", a busca retornou %d documentos.' % (exp_queries[i], len(exp_result[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando os resultados acima, é possível notar que o número de documentos retornados aumentou muito. Acredito que, uma vez que os termos da consulta expandida tenham sim a ver com o termo original, essa abordagem pode ser mais eficiente para recuperar os documentos que mais se relacionam com o que o usuário buscou."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a expansão de consultas gera muito mais documentos retornados, ela é adequada para melhorar o recall, uma vez que o recall é o número de documentos corretos dividido pelo número de documentos que deveriam ter sido retornados. Então quando maior for o número de documentos retornados, maior o recall será."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
