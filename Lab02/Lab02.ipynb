{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - Expansão de Consultas\n",
    "### Hadrizia Santos\n",
    "Nesta atividade será exercitada a noção de expansão de consultas. Considerando a coleção de notícias do lab passado, deve-se executar os seguintes passos:\n",
    "\n",
    "1. Escrever uma função que receba uma coleção de documentos e retorne uma matrix de termos-termos contendo as frequências de co-ocorrência de duas palavras consecutivas no texto (bigramas).\n",
    "2. Escrever uma função que receba um certo termo de consulta e a matriz construída no passo 1 acima e retorneas top-3 palavras em ordem decrescente de frequencia.\n",
    "3. Expandir a consulta original com os termos retornados no passo 2 acima.\n",
    "4. Fazer uma busca disjuntiva (OR) considerando a nova consulta.\n",
    "\n",
    "E responder às perguntas:\n",
    "\n",
    "* Quais os termos retornados para a expansão de cada consulta?\n",
    "* Você acha que esses termos são de fato relacionados com a consulta original? Justifique.\n",
    "* Compare os documentos retornados para a consulta original com a consulta expandida. Quais resultados você acha que melhor capturam a necessidade de informação do usuário? Por que?\n",
    "* A expansão de consultas é mais adequada para melhorar o recall ou o precision? Por que?\n",
    "\n",
    "## Importar bibliotecas e os dados necessários\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from nltk import bigrams   \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy.sparse as sps\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "FILE_PATH = '../data/estadao_noticias_eleicao.csv'\n",
    "\n",
    "df = pd.read_csv(FILE_PATH, encoding = 'utf-8')\n",
    "df = df.replace(np.NAN, \"\")\n",
    "\n",
    "# criação de uma nova coluna para a junção do título da notícia com seu conteúdo\n",
    "noticia = df.titulo + ' ' + df.subTitulo + ' ' + df.conteudo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construir matrix de ocorrência\n",
    "**Obs:** O código abaixo que constrói a matriz de ocorrência foi copiado do repositório que está disponível em: https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# O código abaixo está disponivel em: \n",
    "# https://github.com/allansales/information-retrieval/blob/master/Lab%202/coocurrence_matrix.ipynb\n",
    "\n",
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index\n",
    "\n",
    "def  generate_tokens(noticia):\n",
    "    # Removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_lists = noticia.apply(lambda text: tokenizer.tokenize(text.lower()))\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stopword_ = stopwords.words('portuguese')\n",
    "    filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])\n",
    "    \n",
    "    tokens = [token for tokens_list in filtered_tokens for token in tokens_list]\n",
    "    return tokens\n",
    "\n",
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = generate_tokens(noticia)\n",
    "matrix, vocab = co_occurrence_matrix(tokens)\n",
    "consultable_matrix = matrix.tocsr()\n",
    "inverted_vocab = {vocab[key]: key for key in vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retornar as top-3 palavras mais frequentes de acordo com a matriz de ocorrência\n",
    "Escrever uma função que receba uma palavra do dicionário e retorne quais as 3 palavras que ocorrem mais frequentemente com o input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_co_ocurrences(co_matrix, inv_vocab, term, N=3):\n",
    "    # sparse to dense representation\n",
    "    np_array = np.reshape(co_matrix[term].toarray(), -1)\n",
    "    # get indice of N occurences\n",
    "    return list(OrderedDict({inv_vocab[idx]: np_array[idx] for idx in (-np_array).argsort()[:N]}).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expandir a consulta original com os termos retornados no passo 2 acima.\n",
    "Escrever uma função que receba a consulta, pega as 3 palavras que aparecem com maior frequência junto com esse termo e retorne uma consulta expandida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_query(term):\n",
    "    query = []\n",
    "    query.append(term)\n",
    "    query.extend(get_co_ocurrences(consultable_matrix, inverted_vocab, vocab[term], 3))\n",
    "    return \" \".join(str(x) for x in query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fazer uma busca disjuntiva (OR) considerando a nova consulta.\n",
    "Escrever uma função que recebe uma consulta de N termos e retorna uma consula disjuntiva entre os termos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dilma rousseff disse é\n"
     ]
    }
   ],
   "source": [
    "def or_search(query):\n",
    "    terms = query.lower().split(' ')\n",
    "    for term in terms:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
